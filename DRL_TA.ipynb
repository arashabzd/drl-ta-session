{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DRL_TA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOzS2a65afR01s6L5/DNoDN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arashabzd/drl-ta-session/blob/master/DRL_TA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTwPC0vpzLuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvcHalRFtSDA",
        "colab_type": "text"
      },
      "source": [
        "# Reinforcement Learning Framework\n",
        "\n",
        "How to learn to make good sequences of decisions under uncertainty?\n",
        "\n",
        "<div  align=center>\n",
        "<img alt=\"GAN\" src=\"https://drive.google.com/uc?id=1dntin3evH0GAxNKJKvdE6Yy7Bw3HHP8E\" height=300px />\n",
        "</div>\n",
        "\n",
        "\n",
        "- Agent and environment interact in discrete time steps: $t = 0, 1, 2, ...$\n",
        "- Initial state of the environment is given and may be random: $s_0 \\sim \\mathbb{P}(s_0)$, $s_0 \\in \\mathcal{S}$\n",
        "- Agent observes the environment and determines the state of the environment: $o_t = s_t \\in \\mathcal{S}$ (here we assume that environment is fully observable)\n",
        "- Agent makes a decision on which action to choose in the current state. Agents decision is based on a policy $\\pi$ that is either deterministic ($\\pi: \\mathcal{S} \\rightarrow \\mathcal{A}$) or stochastic ($ \\pi(a_t|s_t)$): $a_t \\in \\mathcal{A}$\n",
        "- State of environment changes based on agents action and environments dynamics: $s_{t+1} \\sim P(s_{t+1}|s_t, a_t )$, $s_{t+1} \\in \\mathcal{S}$\n",
        "- Agent recieves a reward: $r_{t+1} = R(s_t, a_t, s_{t+1}) \\in \\mathbb{R}$ (rewards may be delayed!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jga765B07I_S",
        "colab_type": "text"
      },
      "source": [
        "## Markov Decision Process\n",
        "\n",
        "Two assumptions has been made in this framework that makes it a Markov Decision Process (MDP):\n",
        "\n",
        "1. State transition probabilities satisfy markov property:\n",
        "$\\mathbb{P}(s_{t+1}| a_t, s_t, ..., a_0, s_0) = \\mathbb{P}(s_{t+1}| a_t, s_t)$\n",
        "2. $R(s_0, a_0, ..., s_t, a_t, s_{t+1}) = R(s_t, a_t, s_{t+1})$\n",
        "\n",
        "A Markov Decision Process is defined by a tuple $(\\mathcal{S}, \\mathcal{A}, P, R, s_0)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUD9shPN_kpj",
        "colab_type": "text"
      },
      "source": [
        "## Main Objective\n",
        "\n",
        "The main goal in reinforcement learning is to find a policy that maximizes expected sum of rewards:\n",
        "\n",
        "$$\\pi^* = argmax_\\pi \\mathbb{E}\\left[\\sum_{t=0}^\\infty R(s_t, a_t, s_{t+1}) \\bigg| s_0, \\pi\\right]$$\n",
        "\n",
        "Note that the expectation is on joint distribution of states and actions that by markov property can be factorized as:\n",
        "\n",
        "$$\\prod_{t = 0}^\\infty \\pi(a_t|s_t)P(s_{t+1}| s_t, a_t)$$\n",
        "\n",
        "For a deterministic policy this reduces to:\n",
        "\n",
        "$$\\prod_{t = 0}^\\infty P(s_{t+1}| s_t, \\pi(s_t))$$\n",
        "\n",
        "One problem with this objective is that in infinite horizon setting (where t goes to infinity) this expectation may not converge and we can have multiple policies with infinite expected sum of rewards. In this case comparing different policies is not straight forward and makes optimization hard. For example consider a MDP with two actions, 0 and 1 where each reward is equal to action taken. There are two policies, one that always does the action 1 and one that does 0, 1 in sequence. Both of these policies have infinite expected sum of rewards but one may argue that first policy is better.\n",
        "\n",
        "To alleviate this problem a discount factor $ 0< \\gamma < 1$ can be added to make the summation convergent.\n",
        "\n",
        "$$\\pi^* = argmax_\\pi \\mathbb{E}\\left[\\sum_{t=0}^\\infty \\gamma^t R(s_t, a_t, s_{t+1}) \\bigg| s_0, \\pi\\right]$$\n",
        "\n",
        "This discount factor also reduces the variance of estimating expectation, so it also helps in the finite horizon settings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uor18hMLHyMo",
        "colab_type": "text"
      },
      "source": [
        "## Overview of RL Algorithms\n",
        "\n",
        "\n",
        "<div  align=center>\n",
        "<img alt=\"GAN\" src=\"https://drive.google.com/uc?id=18-G0qRVoJXMI1Zhr6pcfSmop78m8gxc0\" height=300px />\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ4cLo4tW6O5",
        "colab_type": "text"
      },
      "source": [
        "# Dynamic Programming Methods\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfIHnWDmbkbr",
        "colab_type": "text"
      },
      "source": [
        "## Value Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDCfnRYRnmfq",
        "colab_type": "text"
      },
      "source": [
        "### Value Function and Bellman Equation\n",
        "\n",
        "Given a state $s$ and a policy $\\pi$, value of policy $\\pi$ at state $s$ is equal to expected sum of (discounted) rewards starting from $s$ and acting by $\\pi$.\n",
        "\\begin{align*}\n",
        "V^\\pi(s) &= \\mathbb{E}\\left[\\sum_{t=0}^\\infty \\gamma^t R(s_t, a_t, s_{t+1}) \\bigg| s_0=s, \\pi\\right] \\qquad \\text{(Value function)}\\\\\n",
        "&= \\mathbb{E}\\left[R(s_0, a_0, s_1)+\\sum_{t=1}^\\infty \\gamma^t R(s_t, a_t, s_{t+1}) \\bigg| s_0=s, \\pi\\right] \\\\\n",
        "&= \\mathbb{E}\\left[R(s_0, a_0, s_1)+\\gamma V^\\pi(s_1) \\bigg| s_0=s, \\pi\\right] \\\\\n",
        "& = \\sum_{a\\in\\mathcal{A}} \\pi(a|s) \\sum_{s'\\in \\mathcal{S}} P(s'|s, a) \\left[ R(s, a, s') + \\gamma V^\\pi(s')\\right] \\qquad \\text{(Bellman equation)}\n",
        "\\end{align*}\n",
        "\n",
        "For deterministic policies bellman equation reduces to:\n",
        "\\begin{align*}\n",
        "V^\\pi(s) & = \\sum_{s'\\in \\mathcal{S}} P(s'|s, \\pi(s)) \\left[ R(s, \\pi(s), s') + \\gamma V^\\pi(s')\\right]\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHpHYCmogBJV",
        "colab_type": "text"
      },
      "source": [
        "### Optimal Value Function and Optimal Policy\n",
        "\n",
        "Optimal Value functon shows the maximum expected sum of discounted rewards that one can achieve from each state s with optimal play.\n",
        "\n",
        "$$V^*(s) = V^{\\pi^*}(s) = \\max_\\pi V^\\pi(s) \\qquad \\forall s\\in \\mathcal{S}$$\n",
        "\n",
        "Considering partial ordering $\\pi_1 \\geq \\pi_2 \\iff V^{\\pi_1}(s) \\geq V^{\\pi_2}(s) \\quad \\forall s\\in\\mathcal{S}$ it has been proven that:\n",
        "- There exists an optimal policy $\\pi^*$ that is at least as good as all other policies: $\\pi^* \\geq \\pi \\quad \\forall \\pi$\n",
        "- There can be many optimal policies, but all optimal policies achieve the optimal value function.\n",
        "- There is always an optimal deterministic policy for any MDP.\n",
        "- Optimal value function $V^*(s)$ satisfies bellman equation (Bellman Optimality Equation):\n",
        "$$V^*(s) = \\max_{a \\in \\mathcal{A}} \\sum_{s'\\in \\mathcal{S}} P(s'|s, a) \\left[ R(s, a, s') + \\gamma V^*(s')\\right] \\quad \\forall s \\in \\mathcal{S}$$\n",
        "__*__ Any algorithm that solves this __non-linear__ equation can be used to find optimal value function.\n",
        "- Optimal policy can be found using optimal value function:\n",
        "$$\\pi^*(s) = argmax_{a \\in \\mathcal{A}} \\sum_{s'\\in \\mathcal{S}} P(s'|s, a) \\left[ R(s, a, s') + \\gamma V^*(s')\\right] \\quad \\forall s \\in \\mathcal{S}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUA50nm6Uz-h",
        "colab_type": "text"
      },
      "source": [
        "### Value Iteration Algorithm\n",
        "\n",
        "---\n",
        "\n",
        "1. $V_0(s) := 0 \\qquad \\forall s \\in \\mathcal{S}$\n",
        "2. for $k=0, 1, ...$ until convergence:\n",
        "$$V_{k+1}(s) := \\max_{a \\in \\mathcal{A}} \\sum_{s'\\in \\mathcal{S}} P(s'|s, a) \\left[ R(s, a, s') + \\gamma V_k(s')\\right] \\quad \\forall s \\in \\mathcal{S}$$\n",
        "\n",
        "---\n",
        "\n",
        "__*__ On convergence $V_{k+1}(s) \\approx V_{k+1}(s) \\quad \\forall s \\in \\mathcal{S}$.\n",
        "\n",
        "__*__ Value\titeration converges. At convergence, we have found the optimal value function $V^*$ which satisÔ¨Åes the bellman optimality equation.\n",
        "In fact we have:\n",
        "$$\\|V_{i+1} - V_i\\|_\\infty < \\epsilon \\Rightarrow \\|V_{i+1} - V^*\\|_\\infty < 2\\epsilon\\frac{\\gamma}{1-\\gamma} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9bblz6Woni_",
        "colab_type": "text"
      },
      "source": [
        "### Problems\n",
        " - We may not have transition probabilities $P(s'|s, a)$, hence we have to solve a density estimation problem to find transition probabilities. Even if we have transition probabilities or a good model of it, the summation (or integral in case of continuous states) may be intractable.\n",
        " - We can not approximate the expectation using monte-carlo methods due to max."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JulNst4g0mrG",
        "colab_type": "text"
      },
      "source": [
        "### Frozen Lake\n",
        "[FrozenLake](https://github.com/openai/gym/wiki/FrozenLake-v0) is a simple grid-world problem with 16 states and 4 actions. Transition probabilities are known and number of states is tractable so we can easily solve it using value iteration algorithm.\n",
        "\n",
        "The goal of this game is to go from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H). However, the ice is slippery, so you won't always move in the direction you intend (stochastic environment)\n",
        "\n",
        "Here I explain gym environment as I implement the algorithm. For introduction to gym library I suggest you to read the [documentation page](https://gym.openai.com/docs/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsR8lBMvzJpd",
        "colab_type": "code",
        "outputId": "d3eb8c72-29c9-4f3a-dc61-561188c8ac61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# make FrozenLake environment\n",
        "env = gym.make('FrozenLake-v0')\n",
        "\n",
        "# initializes the environment and returns initial state (in FrozenLake initial state is always 0)\n",
        "s = env.reset()\n",
        "\n",
        "# FrozenLake has a discrete observation space that takes 16 values [0-15] corresponding to current position\n",
        "print('observation space:', env.observation_space)\n",
        "\n",
        "# FrozenLake has a discrete action space that takes 4 values [0-3] corresponding to dirrection of movement\n",
        "# 0: Left\n",
        "# 1: Down\n",
        "# 2: Right\n",
        "# 3: Up\n",
        "print('action space:', env.action_space)\n",
        "\n",
        "# transition probabilities\n",
        "# env.P[s][a] = [(p(s'|s, a), s', r, done) for each s' if p(s'|s, a) != 0] :))\n",
        "# done indicates end of an episode\n",
        "print('P[0][2]:', env.P[0][2])\n",
        "\n",
        "# visualizes the environment\n",
        "env.render()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "observation space: Discrete(16)\n",
            "action space: Discrete(4)\n",
            "P[s][a]: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYIRzVZm938L",
        "colab_type": "code",
        "outputId": "e2da734a-c433-43f0-b130-c921c8f4632a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "nS = env.observation_space.n        # number of states\n",
        "nA = env.action_space.n             # number of actions\n",
        "P = env.P\n",
        "\n",
        "gamma = .99                         # discount factor\n",
        "threshold = 1e-9                    # convergence threshold\n",
        "\n",
        "# Value Iteration Algorithm\n",
        "V = np.zeros(nS)\n",
        "while True:\n",
        "    U = np.zeros(nS)\n",
        "    for s in range(nS):\n",
        "        U[s] = np.max([np.sum([t[0] * (t[2] + gamma * V[t[1]]) for t in P[s][a]]) for a in range(nA)])\n",
        "    \n",
        "    if np.linalg.norm(V - U) < threshold:\n",
        "        V = U\n",
        "        break\n",
        "    V = U\n",
        "\n",
        "print('Optimal value function:\\n', V)\n",
        "\n",
        "# extracting policy from value function\n",
        "pi = np.zeros(nS)\n",
        "for s in range(nS):\n",
        "    pi[s] = np.argmax([np.sum([t[0] * (t[2] + gamma * V[t[1]]) for t in P[s][a]]) for a in range(nA)])\n",
        "print('Optimal policy:\\n', pi)\n",
        "\n",
        "\n",
        "# average sum of rewards for 100 episodes\n",
        "rewards = []\n",
        "for i in range(100):\n",
        "    s = env.reset()\n",
        "    done = False\n",
        "    reward = 0.\n",
        "    while not done:\n",
        "        s, r, done, _ = env.step(pi[s])\n",
        "        reward += r\n",
        "    rewards.append(reward)\n",
        "print('average reward:', np.mean(rewards))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimal value function:\n",
            " [0.54202592 0.49880318 0.47069568 0.45685169 0.55845095 0.\n",
            " 0.35834807 0.         0.59179874 0.64307982 0.61520755 0.\n",
            " 0.         0.74172044 0.86283743 0.        ]\n",
            "Optimal policy:\n",
            " [0. 3. 3. 3. 0. 0. 0. 0. 3. 1. 0. 0. 0. 2. 1. 0.]\n",
            "average reward: 0.79\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehg2S49EyZ-R",
        "colab_type": "text"
      },
      "source": [
        "## Q-Value Iteration (Tabular Q-Learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcvLwN2LbwMt",
        "colab_type": "text"
      },
      "source": [
        "### Q-Value Function and Bellman Equation\n",
        "Given a state $s$, an action $a$ and a policy $\\pi$, q-value of policy $\\pi$ at state $s$ starting with action $a$, is equal to expected sum of (discounted) rewards starting from $s$, taking action $a$ and acting by $\\pi$ afterwards.\n",
        "\\begin{align*}\n",
        "Q^\\pi(s, a) &= \\mathbb{E}\\left[\\sum_{t=0}^\\infty \\gamma^t R(s_t, a_t, s_{t+1}) \\bigg| s_0=s, a_0=a, \\pi\\right] \\qquad \\text{(Q-Value function)}\\\\\n",
        "& = \\sum_{s'\\in \\mathcal{S}} P(s'|s, a) \\left[ R(s, a, s') + \\gamma \\sum_{a' \\in \\mathcal{A} }\\pi(a'|s')Q^\\pi(s', a')\\right] \\qquad \\text{(Bellman equation)}\n",
        "\\end{align*}\n",
        "\n",
        "For deterministic policies bellman equation reduces to:\n",
        "\\begin{align*}\n",
        "Q^\\pi(s, a) & = \\sum_{s'\\in \\mathcal{S}} P(s'|s, a) \\left[ R(s, a, s') + \\gamma Q^\\pi(s', \\pi(s'))\\right]\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwRjNoQ3b7Yv",
        "colab_type": "text"
      },
      "source": [
        "### Optimal Q-Value Function and Optimal Policy\n",
        "\n",
        "$$Q^*(s, a) = Q^{\\pi^*}(s, a) = \\max_\\pi Q^\\pi(s, a) \\qquad \\forall s\\in \\mathcal{S}$$\n",
        "\n",
        "- All optimal policies achieve the optimal q-value function..\n",
        "- Optimal q-value function $Q^*(s, a)$ satisfies bellman equation (Bellman Optimality Equation):\n",
        "\\begin{align*}\n",
        "Q^*(s, a) & = \\sum_{s'\\in \\mathcal{S}} P(s'|s, a) \\left[ R(s, a, s') + \\gamma \\max_{a' \\in \\mathcal{A}}Q^*(s', a')\\right]\n",
        "\\end{align*}\n",
        "- Optimal policy can be found using optimal value function:\n",
        "$$\\pi^*(s) = argmax_{a \\in \\mathcal{A}} Q^*(s, a)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_bVpJvncE4Q",
        "colab_type": "text"
      },
      "source": [
        "### Q-Value Iteration Algorithm (Known P)\n",
        "\n",
        "---\n",
        "\n",
        "1. $Q_0(s, a) := 0 \\qquad \\forall (s, a) \\in \\mathcal{S}\\times\\mathcal{A}$\n",
        "2. for $k=0, 1, ...$ until convergence:\n",
        "$$Q^\\pi_{k+1}(s, a) = \\sum_{s'\\in \\mathcal{S}} P(s'|s, a) \\left[ R(s, a, s') + \\gamma \\max_{a' \\in \\mathcal{A}}Q_k^\\pi(s', a')\\right]$$\n",
        "\n",
        "---\n",
        "\n",
        "__*__ On convergence $Q_{k+1}(s, a) \\approx Q_{k}(s, a) \\quad \\forall (s, a) \\in \\mathcal{S}\\times \\mathcal{A}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i74FdaR5eMM",
        "colab_type": "text"
      },
      "source": [
        "### Frozen Lake"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qzee-47-lYuc",
        "colab_type": "code",
        "outputId": "c42a99df-15a8-467b-f025-1d61e74476ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "gamma = .99                         # discount factor\n",
        "threshold = 1e-9                    # convergence threshold\n",
        "\n",
        "# Q-Value Iteration Algorithm\n",
        "Q = np.zeros((nS, nA))\n",
        "while True:\n",
        "    U = np.zeros((nS, nA))\n",
        "    for s in range(nS):\n",
        "        for a in range(nA):\n",
        "            U[s, a] = np.sum([t[0] * (t[2] + gamma * np.max(Q[t[1]])) for t in P[s][a]])\n",
        "    \n",
        "    if np.linalg.norm(Q - U) < threshold:\n",
        "        Q = U\n",
        "        break\n",
        "    Q = U\n",
        "\n",
        "print('Optimal q-value function:\\n', Q)\n",
        "\n",
        "# extracting policy from value function\n",
        "pi = np.argmax(Q, axis=1)\n",
        "\n",
        "print('Optimal policy:\\n', pi)\n",
        "\n",
        "\n",
        "# average sum of rewards for 100 episodes\n",
        "rewards = []\n",
        "for i in range(100):\n",
        "    s = env.reset()\n",
        "    done = False\n",
        "    reward = 0.\n",
        "    while not done:\n",
        "        s, r, done, _ = env.step(pi[s])\n",
        "        reward += r\n",
        "    rewards.append(reward)\n",
        "print('average reward:', np.mean(rewards))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimal q-value function:\n",
            " [[0.54202593 0.52776242 0.52776242 0.52234216]\n",
            " [0.34347361 0.33419813 0.31993462 0.49880318]\n",
            " [0.43818949 0.43362097 0.4243455  0.47069568]\n",
            " [0.30609063 0.30609063 0.30152212 0.45685169]\n",
            " [0.55845096 0.3795824  0.37416214 0.36315737]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.35834807 0.20301849 0.35834807 0.15532958]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.3795824  0.40750993 0.39650516 0.59179874]\n",
            " [0.44006133 0.64307982 0.44778624 0.39831208]\n",
            " [0.61520756 0.49695269 0.40299121 0.3304712 ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.45698409 0.5295041  0.74172044 0.49695269]\n",
            " [0.73252259 0.86283743 0.82108818 0.78111957]\n",
            " [0.         0.         0.         0.        ]]\n",
            "Optimal policy:\n",
            " [0 3 3 3 0 0 0 0 3 1 0 0 0 2 1 0]\n",
            "average reward: 0.77\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QkHtmv-sNVt",
        "colab_type": "text"
      },
      "source": [
        "### Q-Value Iteration Algorithm (Unknown P)\n",
        "\n",
        "We can estimate the expectation using a one-sample monte-carlo estimate by choosing an action and observing next state of the environment by easily changing algorithm to:\n",
        "\n",
        "---\n",
        "\n",
        "1. $Q(s, a) := 0 \\qquad \\forall (s, a) \\in \\mathcal{S}\\times\\mathcal{A}$\n",
        "2. for episode $i = 1, 2, ..., N$:\n",
        "    - for $t = 0, 1, ...$:\n",
        "        - choose an action $a_t$ ($\\epsilon-greedy$)\n",
        "        - observe next state $s_{t+1}$\n",
        "        - if $s_{t+1}$ is terminal:\n",
        "            - $Q(s_t, a_t) = R(s_t, a_t, s_{t+1})$\n",
        "        - else:\n",
        "            - $Q(s_t, a_t) = R(s_t, a_t, s_{t+1}) + \\gamma \\max_{a' \\in \\mathcal{A}}Q(s_{t+1}, a')$\n",
        "\n",
        "---\n",
        "\n",
        "__*__ One-sample approximation of expectation is very high variane. To reduce this variance we can incorporate Q-values in a running average:\n",
        " $$Q(s_t, a_t) = (1-\\alpha_t) Q(s,a) + \\alpha_t \\left[R(s_t, a_t, s_{t+1}) + \\gamma \\max_{a' \\in \\mathcal{A}}Q(s_{t+1}, a')\\right]$$\n",
        " Parameter $\\alpha$ (learning rate) should satisfy following conditions:\n",
        " $$\\sum_k \\alpha_k^2 < L, \\quad \\sum_k \\alpha_k = \\infty$$\n",
        " This method is also called __Temporal Difference Learning__.\n",
        " We will motivate this more later.\n",
        "\n",
        " __*__ $\\epsilon-greedy$ strategy: Choose random action with probability $\\epsilon$, otherwise choose action greedily. \tQ-learning converges to optimal policy even if you‚Äôre acting suboptimally (off-policy learning). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEYzMu5ncNRR",
        "colab_type": "text"
      },
      "source": [
        "### Frozen Lake\n",
        "\n",
        "Here we do not use transition probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3k3CKqirkYXg",
        "colab_type": "code",
        "outputId": "05d4fa9f-b911-4857-88df-739ef5e1241e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "epsilon = .2\n",
        "alpha = .5\n",
        "gamma = .99\n",
        "Q = np.zeros((nS, nA))\n",
        "\n",
        "for i in range(20000):\n",
        "    s = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        \n",
        "        if np.random.rand() < epsilon:\n",
        "            a = env.action_space.sample()\n",
        "        else:\n",
        "            a = np.argmax(Q[s])\n",
        "        \n",
        "        s_, r, done, _ = env.step(a)\n",
        "\n",
        "        if done:\n",
        "            target = r\n",
        "        else:\n",
        "            target = r + gamma * np.max(Q[s_])\n",
        "        \n",
        "        Q[s, a] = (1 - alpha) * Q[s, a] + alpha * target\n",
        "        s = s_\n",
        "\n",
        "# extracting policy from value function\n",
        "pi = np.argmax(Q, axis=1)\n",
        "print('Optimal policy:\\n', pi)\n",
        "\n",
        "# average sum of rewards for 100 episodes\n",
        "rewards = []\n",
        "for i in range(100):\n",
        "    s = env.reset()\n",
        "    done = False\n",
        "    reward = 0.\n",
        "    while not done:\n",
        "        s, r, done, _ = env.step(pi[s])\n",
        "        reward += r\n",
        "    rewards.append(reward)\n",
        "print('average reward:', np.mean(rewards))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimal policy:\n",
            " [0 2 3 3 0 0 2 0 3 1 0 0 0 2 1 0]\n",
            "average reward: 0.63\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRzxcdDOgk8i",
        "colab_type": "text"
      },
      "source": [
        "### Cart Pole\n",
        "\n",
        "In [CartPole](https://github.com/openai/gym/wiki/CartPole-v0) problem we do not know the transition probabilities and observation space is continuous hence we have to estimate the expectation and discretize the state. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlrtVw1hFCML",
        "colab_type": "code",
        "outputId": "948022b2-7a68-47c4-cfd9-87a8be07871e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "epsilon = .2\n",
        "alpha = .5\n",
        "gamma = .99\n",
        "Q = np.zeros((2, 2, 2, 2, 2))\n",
        "\n",
        "for i in range(20000):\n",
        "    s = (env.reset() > 0).astype(int)   # simplest case of discretization\n",
        "    done = False\n",
        "    while not done:\n",
        "        if np.random.rand() < epsilon:\n",
        "            a = env.action_space.sample()\n",
        "        else:\n",
        "            a = np.argmax(Q[s[0], s[1], s[2], s[3]])\n",
        "        \n",
        "        s_, r, done, _ = env.step(a)\n",
        "        s_ = (s_ > 0).astype(int)\n",
        "\n",
        "        if done:\n",
        "            target = r\n",
        "        else:\n",
        "            target = r + gamma * np.max(Q[s_[0], s_[1], s_[2], s_[3]])\n",
        "        \n",
        "        Q[s[0], s[1], s[2], s[3], a] = (1 - alpha) * Q[s[0], s[1], s[2], s[3], a] + alpha * target\n",
        "        s = s_\n",
        "\n",
        "pi = np.argmax(Q, axis=4)\n",
        "rewards = []\n",
        "for i in range(100):\n",
        "    s = (env.reset() > 0).astype(int)\n",
        "    done = False\n",
        "    reward = 0.\n",
        "    while not done:\n",
        "        s, r, done, _ = env.step(pi[s[0], s[1], s[2], s[3]])\n",
        "        s = (s > 0).astype(int)\n",
        "        reward += r\n",
        "    rewards.append(reward)\n",
        "print('average reward:', np.mean(rewards))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average reward: 42.72\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLmrPTO8sWIc",
        "colab_type": "text"
      },
      "source": [
        "### Problems\n",
        "- Needs a lot of episodes and exploration for a good approximation.\n",
        "- If we do not see a state action pair during training, we dont have any idea about it (We do not __generalize__).\n",
        "- We need a table of size |$\\mathcal{S}|\\times|\\mathcal{A}|$ to store q-values. We can not scale well to high dimensional continuous state or action spaces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st2_7ybx0GAr",
        "colab_type": "text"
      },
      "source": [
        "# Approximate Q-Learning\n",
        "\n",
        "In realistic situations, we cannot possibly learn about every single state:\n",
        "- Too\tmany\tstates\tto\tvisit\tthem\tall\tin\ttraining \n",
        "- Too\tmany\tstates\tto\thold\tthe\tq-tables\tin\tmemory \n",
        "\n",
        "Instead,\twe\twant\tto\tgeneralize: \n",
        "-  Learn\tabout\tsome\tsmall\tnumber\tof\ttraining\tstates\tfrom\texperience \n",
        "- Generalize\tthat\texperience\tto\tnew,\tsimilar\tsituations \n",
        "\n",
        "What if instead of keeping a table of all q-values, we approximate q-value function with a function approximator $Q_\\theta: \\mathcal{S}\\times\\mathcal{A} \\rightarrow\\mathbb{R}$? In this way we can **generalize** between similar state action values and **we do not have to keep a large q-value table**. Now the problem is how to find the parameters?\n",
        "\n",
        "Recall that optimal Q-Vlaue function satisfies bellamn equation:\n",
        "\n",
        "\\begin{align*}\n",
        "Q^*(s, a) &=  \\sum_{s' \\in \\mathcal{S}} P(s' | s, a) \\left[R(s, a, s') + \\gamma \\max_{a' \\in \\mathcal{A}}Q^*(s', a')\\right]\\\\\n",
        "&\\approx R(s, a, s') + \\gamma \\max_{a' \\in \\mathcal{A}}Q^*(s', a') \\qquad s'\\sim P(s'|s, a)\n",
        "\\end{align*}\n",
        "\n",
        "We want our parametric function $Q_\\theta$ be as close as possible to $Q^*$. We can find parameters such that $Q_\\theta(s, a)$ be as close as possible to  $target=R(s, a, s') + \\gamma \\max_{a' \\in \\mathcal{A}}Q^*(s', a')$ for each $s'\\sim P(s'|s, a)$. To compute target value we must calculate \n",
        "$\\max_{a' \\in \\mathcal{A}}Q^*(s', a')$ but we do not have $Q^*$ so instead we put our best approximation of it (i.e $Q_\\theta$). We can minimize a squared error between our $Q_\\theta$ and  target values.\n",
        "\n",
        "---\n",
        "\n",
        "1. Initialize $Q_\\theta(s, a)$\n",
        "2. for episode $i = 1, 2, ..., N$:\n",
        "    - observe state $s_0$\n",
        "    - for $t = 0, 1, ...$:\n",
        "        - choose an action $a_t$ ($\\epsilon-greedy$)\n",
        "        - observe next state $s_{t+1}$\n",
        "        - if $s_{t+1}$ is terminal:\n",
        "            - $target = R(s_t, a_t, s_{t+1})$\n",
        "        - else:\n",
        "            - $target = R(s_t, a_t, s_{t+1}) + \\gamma \\max_{a' \\in \\mathcal{A}}\n",
        "            Q_\\theta(s_{t+1}, a')$\n",
        "\n",
        "        - $\\theta = \\theta - \\alpha  \\nabla_\\theta  \\frac{1}{2}\\left[Q_\\theta(s_t, a_t) - target\\right]^2$\n",
        "        - $s_t = s_{t+1}$\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZagtjgmR0ARN",
        "colab_type": "text"
      },
      "source": [
        "## Cartpole with Polynomial Function Approximator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK5kus6ZiHVW",
        "colab_type": "code",
        "outputId": "6a4a2cf0-244a-4e1d-aabd-0410e8d93205",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "nA = env.action_space.n\n",
        "\n",
        "epsilon = .2\n",
        "alpha = .01\n",
        "gamma = .99\n",
        "\n",
        "theta = np.random.randn(2, 9)\n",
        "\n",
        "# features\n",
        "def f(s):\n",
        "    return np.concatenate([np.ones(1), s, s**2])\n",
        "\n",
        "# parametrized Q-function\n",
        "def Q(theta, s):\n",
        "    return np.dot(theta, f(s))\n",
        "\n",
        "# policy\n",
        "def pi(theta, s):\n",
        "    return np.argmax(Q(theta, s))\n",
        "\n",
        "for i in range(20000):\n",
        "    s = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        if np.random.rand() < epsilon:\n",
        "            a = env.action_space.sample()\n",
        "        else:\n",
        "            a = pi(theta, s)\n",
        "        \n",
        "        s_, r, done, _ = env.step(a)\n",
        "\n",
        "        if done:\n",
        "            target = r\n",
        "        else:\n",
        "            target = r + gamma * np.max(Q(theta, s_))\n",
        "        \n",
        "        theta -= alpha * (Q(theta, s)[a] - target) * f(s) \n",
        "        s = s_\n",
        "\n",
        "rewards = []\n",
        "for i in range(100):\n",
        "    s = env.reset()\n",
        "    done = False\n",
        "    reward = 0.\n",
        "    while not done:\n",
        "        s, r, done, _ = env.step(pi(theta, s))\n",
        "        reward += r\n",
        "    rewards.append(reward)\n",
        "print('average reward:', np.mean(rewards))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average reward: 22.66\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7p_-fwvGtq-",
        "colab_type": "text"
      },
      "source": [
        "## Tabular Q-Learning as a special case of Approximate Q-Learning\n",
        "\n",
        "Assume our function approximator is in the form:\n",
        "\n",
        "$$Q_\\theta(s, a) = \\theta_{s, a} \\qquad \\theta \\in \\mathbb{R}^{|\\mathcal{S}| \\times |\\mathcal{A}|}$$\n",
        "Therefore we have:\n",
        "\n",
        "\\begin{align*}\n",
        "\\theta_{s, a} &= \\theta_{s, a} - \\alpha \\nabla_{\\theta_{s, a}}  \\frac{1}{2}\\left[\\theta_{s, a} - target\\right]^2 \\\\\n",
        "& = \\theta_{s, a} - \\alpha  \\left(\\theta_{s, a} - target\\right)\\\\\n",
        "& = (1- \\alpha)\\theta_{s, a} + \\alpha ~ (target)\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30r0uG-dQQsS",
        "colab_type": "text"
      },
      "source": [
        "# DQN\n",
        "\n",
        "The main Idea is to **use a Neural Network as function approximator** in Approximate Q-Learning. Two main ideas has been used to stabilize Q-learning:\n",
        "\n",
        "1. Apply Q-updates on batches of past experience instead of online (Use **Experience Replay**): Instead of using a one-sample monte-carlo estimate of expectation (online updates), store transitions $(s, a, s', r)$ in a buffer called experience replay and update parameters on a batch of transitions to reduce variance. This makes data distribution more stationary since data samples are not highly correlated due to online updates.\n",
        "2. In supervised learning targets are constatnt (do not depend on parameters $\\theta$), but in Q-Learning computed targets are dependent on parameters $\\theta$ and change every time that $\\theta$ changes. To prevent target values from changing too fast we can use a target network that its parameters change less often $Q_{\\theta^-}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XY53Ha9OH1Y",
        "colab_type": "text"
      },
      "source": [
        "## DQN Algorithm\n",
        "\n",
        "---\n",
        "\n",
        "1. Initialize $Q_\\theta(s, a)$\n",
        "2. $Q_{\\theta^-} = Q_{\\theta}$\n",
        "3. for episode $i = 1, 2, ..., N$:\n",
        "    - observe state $s_0$\n",
        "    - for $t = 0, 1, ...$:\n",
        "        - choose an action $a_t$ ($\\epsilon-greedy$)\n",
        "        - observe next state $s_{t+1}$\n",
        "        - add $(s_t, a_t, s_{t+1}, r_{t+1})$ to experience replay\n",
        "        - take a batch $ \\left[(s_i, a_i, s'_i, r_i)\\right]_i$ from experience replay\n",
        "        - if $s'_i$ is terminal:\n",
        "            - $target_i = r_i$\n",
        "        - else:\n",
        "            - $target_i = r_i + \\gamma \\max_{a' \\in \\mathcal{A}}\n",
        "            Q_{\\theta^-}(s'_i, a')$\n",
        "\n",
        "        - $\\theta = \\theta - \\alpha  \\nabla_\\theta  \\frac{1}{2} \\sum_i \\left[Q_\\theta(s_i, a_i) - target_i\\right]^2$\n",
        "        - $s_t = s_{t+1}$\n",
        "\n",
        "    - $Q_{\\theta^-} = Q_\\theta$\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj5Lyxh3QAEr",
        "colab_type": "text"
      },
      "source": [
        "# Policy Optimization\n",
        "\n",
        "The approach we have taken so far to solve RL problem is to first find Value function or Q-Value function using any variant of bellman equation, then extract the policy corresponding to our learned Value or Q-Value function. Finding optimal Q-value for each state-action pair in a complex environment, is a very hard problem. What if we directly learn a parametrized policy $\\pi_\\theta$? We don't need to exactly know $Q(s, a)$ for every state-action pair to determine the correct action, we only need to know which action maximizes $Q(s, a)$ for every state and the exact value is irrelevant. By this we can say that policy function is simpler that Q-Value function, and learning policy directly could be easier.\n",
        "\n",
        "<div  align=center>\n",
        "<img alt=\"GAN\" src=\"https://drive.google.com/uc?id=1DlFcYIyU4oEWVV-aQSxyOM9iFqJzars4\" height=300px />\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlOvWuVHQXH_",
        "colab_type": "text"
      },
      "source": [
        "## Policy Gradient\n",
        "\n",
        "Recall the main objective stated earlier, let's rewrite that for a parametrized policy $\\pi_\\theta$:\n",
        "\n",
        "$$\\theta^* = argmax_\\theta \\mathbb{E}\\left[\\sum_{t=0}^\\infty R(s_t, a_t, s_{t+1}) \\bigg| s_0, \\pi_\\theta\\right]$$\n",
        "Where the expectation is on:\n",
        "$$\\prod_{t = 0}^\\infty \\pi_\\theta(a_t|s_t)P(s_{t+1}| s_t, a_t)$$\n",
        "\n",
        "Let's try to optimize this objective. Exact computation of this expectation is intractable and impossible when we don't have environments dynamics $P$. We can approximate this expectation using monte-carlo method:\n",
        "\n",
        "$$\\theta^* = argmax_\\theta \\frac{1}{N}\\sum_{i=1}^N\\sum_{t=0}^\\infty R(s^i_t, a^i_t, s^i_{t+1})$$\n",
        "\n",
        "where $a^i_t \\sim \\pi_\\theta(a_t|s^i_t)$, $s^i_{t+1}\\sim P(s_{t+1} | s^i_t, a^i_t)$ and $s^i_0$ is given for all i.\n",
        "\n",
        "We can't optimize this objective since it does not depend on $\\theta$ so its gradient is zero and gradient descent can not work. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs_7gn_G6ogv",
        "colab_type": "text"
      },
      "source": [
        "### Reparametrization Trick (Pathwise Derivatives)\n",
        "\n",
        "One way to solve this problem is to use reparametrization trick. In general assume we want optimize $\\theta$ in the objective of the form $\\mathbb{E}_{p_\\theta(x)}[f(x)]$. In reparametrization trick we find a differentiable (w.r.t. parameters) and invertible transformation $g$ and a random variable $\\epsilon$ with distribution $p(\\epsilon)$ that is independent of $\\theta$ such that $x = g(\\epsilon; \\theta)$. Then we have:\n",
        "\n",
        "$$\\mathbb{E}_{p_\\theta(x)}[f(x)] = \\mathbb{E}_{p(\\epsilon)}[f(g(\\epsilon; \\theta))] \\approx \\frac{1}{N}\\sum_{i=1}^N f(g(\\epsilon^i; \\theta))$$\n",
        "\n",
        "Let's assume we have such $g$ and  $\\epsilon$ and apply this trick to our objective:\n",
        "\n",
        "$$\\theta^* = argmax_\\theta \\frac{1}{N}\\sum_{i=1}^N\\sum_{t=0}^\\infty R(s^i_t, g(\\epsilon^i_t;s^i_t, \\theta), s^i_{t+1})$$\n",
        "\n",
        "where $\\epsilon^i_t \\sim p(\\epsilon)$, $s^i_{t+1}\\sim P(s_{t+1} | s^i_t, g(\\epsilon^i_t;s^i_t, \\theta))$ and $s^i_0$ is given for all i.\n",
        "This objective does depend on $\\theta$. We can optimize this **if R is differentiable w.r.t. $g$**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvesXRCSFTFW",
        "colab_type": "text"
      },
      "source": [
        "### REINFORCE (Score Function Estimator)\n",
        "\n",
        "R is not always differentiable hence we can't use reparametrization trick. Reinforce algorithm introduces another solution for this problem.\n",
        "\n",
        "Let's rewite our objective with a simpilified notation. Let $\\tau = s_0, a_0, s_1, a_1 ,...$, $p_\\theta(\\tau) = \\prod_{t = 0}^\\infty \\pi_\\theta(a_t|s_t)P(s_{t+1}| s_t, a_t)$ and $r(\\tau) = \\sum_{t=0}^\\infty R(s_t, a_t, s_{t+1})$then we have:\n",
        "\n",
        "\\begin{align*}\n",
        "\\nabla_\\theta\\mathbb{E}\\left[\\sum_{t=0}^\\infty R(s_t, a_t, s_{t+1}) \\bigg| s_0, \\pi_\\theta\\right] \n",
        "&= \\nabla_\\theta\\mathbb{E}_{p_\\theta(\\tau)}\\left[r(\\tau)\\bigg| s_0, \\pi_\\theta\\right] \\\\\n",
        "&=\\nabla_\\theta\\int_\\tau p_\\theta(\\tau)r(\\tau)~d\\tau\\\\\n",
        "&=\\int_\\tau p_\\theta(\\tau)\\frac{\\nabla_\\theta p_\\theta(\\tau)}{p_\\theta(\\tau)}r(\\tau)~d\\tau\\\\\n",
        "&=\\int_\\tau p_\\theta(\\tau)\\nabla_\\theta \\log p_\\theta(\\tau) r(\\tau)~d\\tau\\\\\n",
        "&=\\mathbb{E}_{p_\\theta(\\tau)} \\left[\\nabla_\\theta \\log p_\\theta(\\tau) r(\\tau)\\right]\\\\\n",
        "&\\approx \\frac{1}{N}\\sum_{i=1}^N \\nabla_\\theta \\log p_\\theta(\\tau^i) r(\\tau^i)\n",
        "\\end{align*}\n",
        "\n",
        "Now we only have to calculate $\\nabla_\\theta \\log p_\\theta(\\tau)$:\n",
        "\n",
        "\\begin{align*}\n",
        "\\nabla_\\theta \\log p_\\theta(\\tau) &= \\nabla_\\theta \\log \\left(\\prod_{t=0}^\\infty \\pi_\\theta(a_t|s_t)P(s_{t+1}|s_t, a_t)\\right) \\\\\n",
        "&= \\nabla_\\theta \\sum_{t = 0}^\\infty \\log \\pi_\\theta(a_t|s_t) + \\log P(s_{t+1}| s_t, a_t) \\\\\n",
        "&=  \\sum_{t = 0}^\\infty \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\n",
        "\\end{align*}\n",
        "\n",
        "Fortunately this **doesn't depend on transition probabilities**!\n",
        "\n",
        "__*__ Likelihood\tratio\tchanges\tprobabilities\tof\texperienced\tpaths,\tdoes\tnot\ttry\tto\tchange\tthe\tpaths (Pathwise Derivative does)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvjV35_vU-Yf",
        "colab_type": "text"
      },
      "source": [
        "#### REINFORCE Algorithm\n",
        "\n",
        "---\n",
        "\n",
        "1. Initialize $\\pi_\\theta$\n",
        "2. for iterations $i = 1, 2, ...$:\n",
        "    - Rollout N episodes with policy $\\pi_\\theta$ and save $(s^i_t, a^i_t, s^i_{t+1}, r^i_{t+1})$\n",
        "\n",
        "    - Compute $\\mathcal{L} = -\\frac{1}{N}\\sum_{i=1}^N \\left( \\sum_{t = 0}^\\infty\\log \\pi_\\theta(a^i_t|s^i_t) \\right) \\left(\\sum_{t'=0}^\\infty R(s^i_{t'}, a^i_{t'}, s^i_{t'+1}) \\right)$\n",
        "    - BackProp and minimize $\\mathcal{L}$ using GD.\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMt3Q2lZQjgA",
        "colab_type": "text"
      },
      "source": [
        "## Actor-Critic Methods\n",
        "\n",
        "Policy gradient objective tries to:\n",
        "- Increase probability of paths\twith positive R\n",
        "- Decrease probability of paths with negative R\t\t\n",
        "\n",
        "What if we only get positive (negative) rewards?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNCFSPuaYEeU",
        "colab_type": "text"
      },
      "source": [
        "### Baseline\n",
        "Baseline tries to solve the positive (negative) reward problem with subtracting a baseline (mean reward) from path reward. Another motivation of this baseline is to reduce variance (You will learn about this in your homework).\n",
        "Gradient estimate using a baseline becomes:\n",
        "\n",
        "$$\\frac{1}{N}\\sum_{i=1}^N \\nabla_\\theta \\log p_\\theta(\\tau^i) \\left(r(\\tau^i) - b\\right)$$\n",
        "\n",
        "This estimate is unbiased as long as $b$ doesn't depend on actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxLMGxq_e9Zp",
        "colab_type": "text"
      },
      "source": [
        "#### Popular Baselines\n",
        "\n",
        "- Constant Baseline: $b = \\mathbb{E}[r(\\tau)]$\n",
        "\n",
        "- Optimal Constant Baseline: $b = \\frac{\\mathbb{E}[(\\nabla_\\theta \\log p_\\theta (\\tau))^2 r(\\tau)]}{\\mathbb{E}[\\nabla_\\theta \\log p_\\theta (\\tau))^2]}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5PgidaDYHDC",
        "colab_type": "text"
      },
      "source": [
        "### Temporal Structure\n",
        "\n",
        "In the original policy gradient objective we increase or decrease probability of an action depending on the whole path reward. But instead we want to modify the probability of an action by rewards that are consequence of that action. We can do this by taking temporal structure of rewards into account:\n",
        "\n",
        "$$\\frac{1}{N}\\sum_{i=1}^N \\left( \\sum_{t = 0}^\\infty \\nabla_\\theta \\log \\pi_\\theta(a^i_t|s^i_t) \\left(\\sum_{t'=t}^\\infty R(s^i_{t'}, a^i_{t'}, s^i_{t'+1}) - b\\right) \\right)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JTqezLhfBot",
        "colab_type": "text"
      },
      "source": [
        "#### Popular Baselines\n",
        "\n",
        "- Time Dependent Baseline: $b_t = \\mathbb{E}[\\sum_{t'=t}^\\infty R(s_{t'}, a_{t'}, s_{s'+1})]$\n",
        "\n",
        "- State Dependent Baseline: $b(s_t) = V^{\\pi}(s_t) = \\mathbb{E}\\left[\\sum_{t'=0}^\\infty R(s_{t'}, a_{t'}, s_{t'+1}) \\bigg| s_0=s_t, \\pi\\right]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWuJV54thpKL",
        "colab_type": "text"
      },
      "source": [
        "### Advantage Actor Critic Method (A2C)\n",
        "\n",
        "Main idea of this method is to increase or decrease probability of an action based on __Advantage__ of that action under current policy. We already tried to find the advantage of an action by different baselines and sum of rewards $\\left(\\sum_{t'=t}^\\infty R(s^i_{t'}, a^i_{t'}, s^i_{t'+1}) - b\\right)$ but the optimal way to calculate advantage is to use Value / Q-Value functions:\n",
        "\n",
        "\\begin{align*}\n",
        "A^{\\pi_\\theta}(s_t, a_t) &= Q^{\\pi_\\theta}(s_t, a_t) - V^{\\pi_\\theta}(s_t)\\\\\n",
        "\\end{align*}\n",
        "\n",
        "One way to find this advantage is to use another function approximator for Value function $V^{\\pi_\\theta}_\\phi$ and find Q-Value according to K-step look ahead equation:\n",
        "\n",
        "\\begin{align*}\n",
        "Q^{\\pi_\\theta}_\\phi(s, a) &= \\mathbb{E}\\left[\\sum_{t=0}^{K-1}R(s_{t}, a_{t}, s_{t+1}) + V^{\\pi_\\theta}_\\phi(s_{K}) \\bigg|s_0=s, a_0=a, \\pi_\\theta \\right]\\\\\n",
        "&\\approx \\sum_{t=0}^{K-1}R(s_{t}, a_{t}, s_{t+1}) + V^{\\pi_\\theta}_\\phi(s_{K}) \\qquad \\text{(one-sample approximation)}\n",
        "\\end{align*}\n",
        "\n",
        "For K = 1:\n",
        "\\begin{align*}\n",
        "Q^{\\pi_\\theta}_\\phi(s, a) &\\approx R(s, a, s') + V^{\\pi_\\theta}_\\phi(s') \n",
        "\\end{align*}\n",
        "\n",
        "Now the question is how to fit $V^{\\pi_\\theta}_\\phi$? Answer: Bellman Equation!\n",
        "Recall Bellman Equation for value function:\n",
        "\n",
        "\\begin{align*}\n",
        "V^{\\pi_\\theta}_\\phi(s) & = \\mathbb{E} \\left[ R(s, a, s') + V^{\\pi_\\theta}_\\phi(s') \\bigg |\\pi_\\theta\\right]\n",
        "\\end{align*}\n",
        "\n",
        "We can use this equation to fit a parametrized value function the same way we did for Approximate Q-Learning, by minimizing a squared error between $target=R(s, a, s') +  V^{\\pi_\\theta}_\\phi(s')$ and our value function $[target - V^{\\pi_\\theta}_\\phi(s)]^2$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USsNBrpuyesS",
        "colab_type": "text"
      },
      "source": [
        "#### A2C Algorithm\n",
        "---\n",
        "\n",
        "1.   Sample a batch $\\{(s_i, a_i, r_i, s_{i + 1})\\}_i$ under policy $\\pi_\\theta$.\n",
        "2.   Fit $V_\\phi^{\\pi_\\theta}(s_i)$ to $r_i + \\gamma V_\\phi^{\\pi_\\theta}(s_{i+1})$ by minimizing squared error $\\|r_i + \\gamma V_\\phi^{\\pi_\\theta}(s_{i+1})- V_\\phi^{\\pi_\\theta}(s_i)\\|^2$.\n",
        "3. $\\max_{\\theta}~ \\sum_{i} \\log \\pi_\\theta(a_i|s_i) \\left[ r_i + \\gamma V_\\phi^{\\pi_\\theta}(s_{i+1})- V^{\\pi_\\theta}_\\phi(s_i) \\right]$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iVr0WwfaHwJ",
        "colab_type": "text"
      },
      "source": [
        "# references\n",
        "\n",
        "- [Deep RL Bootcamp](https://sites.google.com/view/deep-rl-bootcamp/lectures)\n"
      ]
    }
  ]
}